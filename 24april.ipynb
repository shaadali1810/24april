{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1e2d1f-f414-4703-95ca-e7ef2cb61c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1However, what we really want is the projection of the original data onto the new dimension. The last step of PCA is we need to multiply Q tranpose of Q with the original data matrix in order to get the projection matrix. We go from the (d x k) Q matrix and Q transpose of Q results in d x d dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6248a6c-1fdb-4b94-b193-de325e166856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2PCA can be used to reduce the dimensionality of the data by creating a set of derived variables that are linear combinations of the original variables. The values of the derived variables are given in the columns of the scores matrix Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3f5b8f-56a8-4460-955e-bb061fa761d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3PCA is simply described as “diagonalizing the covariance matrix”. What does diagonalizing a matrix mean in this context? It simply means that we need to find a non-trivial linear combination of our original variables such that the covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f90f6cd-59c5-4125-a09d-8ff2b79194bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32f34d3-f24b-423b-bba3-237f0436141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5PCA reduces the dimensionality without losing information from any features. Speed up the learning algorithm (with lower dimension). Address the multicollinearity issue (all principal components are orthogonal to each other). Help visualize data with high dimensionality (after reducing the dimension to 2 or 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d051de8-7085-47a4-9b9b-c00c6d08fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#PCA is used to visualize multidimensional data.\n",
    "#It is used to reduce the number of dimensions in healthcare data.\n",
    "#PCA can help resize an image.\n",
    "#It can be used in finance to analyze stock data and forecast returns.\n",
    "#PCA helps to find patterns in the high-dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e496-d9c4-4bb9-a376-93b0ca8fd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
